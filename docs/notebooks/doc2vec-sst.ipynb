{
 "metadata": {
  "name": "",
  "signature": "sha256:467ce8c810f4e38ad82a4ca7ee3329a91c61559b41df4e09653708719fd85a0c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "gensim doc2vec & Stanford Sentiment Treebank"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# while developing, auto-reload changed source\n",
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data available from http://nlp.Stanford.edu/sentiment/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.corpora import susentcorpus\n",
      "corpus = susentcorpus.StanfordSentimentCorpus('stanfordSentimentTreebank')\n",
      "alldocs = list(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sort by sentiment-contrast\n",
      "# most neg, least neg, most pos, least pos\n",
      "#corpus.phrases.sort(key=lambda phrase: phrase.sentiment if phrase.sentiment<0.5 else (0.5+(1-phrase.sentiment)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set-up Training & Evaluation Models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[p.sentiment for p in corpus.phrases][100:110]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[0.65278,\n",
        " 0.54167,\n",
        " 0.69444,\n",
        " 0.77778,\n",
        " 0.13889,\n",
        " 0.055556,\n",
        " 0.22222,\n",
        " 0.19444,\n",
        " 0.5,\n",
        " 0.54167]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), 400 dimensions of each model, \"PV-DM\" and \"PV-DBOW\". "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Doc2Vec\n",
      "from collections import OrderedDict\n",
      "\n",
      "models_by_name = OrderedDict([\n",
      "   ('dbow_100_neg5', [Doc2Vec(dm=0,size=100,window=10,negative=5,hs=0,min_count=1,workers=4,seed=1)]),\n",
      "   ('dbow_100_neg5hs', [Doc2Vec(dm=0,size=100,window=10,negative=5,hs=1,min_count=1,workers=4,seed=1)]),\n",
      "   ('dm_100_neg5', [Doc2Vec(dm=1,size=100,window=10,negative=5,hs=0,min_count=1,workers=4,seed=2)]),\n",
      "   ('dm_100_neg5hs', [Doc2Vec(dm=1,size=100,window=10,negative=5,hs=1,min_count=1,workers=4,seed=4)]),\n",
      "])\n",
      "\n",
      "\n",
      "#dbow_model_neg = Doc2Vec(dm=0,size=400,hs=0,negative=15,min_count=1,workers=4,seed=1)\n",
      "#dbow_model_neg_preload = Doc2Vec(dm=0,size=400,hs=0,negative=15,min_count=1,workers=4,seed=1)\n",
      "#dbow_model_hs = Doc2Vec(dm=0,size=400,hs=1,negative=0,min_count=1,workers=4,seed=1)\n",
      "#dm_model_neg = Doc2Vec(dm=1,dm_mean=1,size=300,hs=0,negative=15,min_count=1,window=5,workers=4,seed=2)\n",
      "#dm_model_neg_preload = Doc2Vec(dm=1,dm_mean=1,size=300,hs=0,negative=15,min_count=1,window=5,workers=4,seed=2)\n",
      "#dm_model_neg_punlocked = Doc2Vec(dm=1,dm_mean=1,size=300,hs=0,negative=15,min_count=1,window=5,workers=4,seed=2)\n",
      "\n",
      "#dm_model_hs = Doc2Vec(dm=1,dm_mean=1,size=400,hs=1,negative=0,min_count=1,window=5,workers=4,seed=2)\n",
      "\n",
      "for model_list in models_by_name.values():\n",
      "    [model.build_vocab(alldocs) for model in model_list]\n",
      "    \n",
      "\n",
      "# dm_model = Doc2Vec(dm=1,dm_mean=1,size=400,hs=0,negative=30,min_count=1,window=5,workers=4,seed=2) # dm_mean=1 gives more sensible word similarities, faster improvement per pass on sentiment predictions\n",
      "# dm_concat model with giant layer1 uses most RAM: ~5GB\n",
      "# dm_concat model not yet cythonized/BLAS-enabled: takes 60x longer per training pass\n",
      "#dmc_model = Doc2Vec(dm=1,dm_concat=1,size=400,hs=1,negative=0,min_count=1,window=7,workers=4,seed=3)\n",
      "\n",
      "# note: using different seeds seems to help prevent \"LinAlgError: Singular Matrix\" during *some* \n",
      "# regressions on concatenated-models, perhaps due to certain doc vectors repeating exactly? "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if more visual progress wanted in vocab/training steps, uncomment\n",
      "#import logging\n",
      "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "#rootLogger = logging.getLogger()\n",
      "#rootLogger.setLevel(logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Prep vocabulary. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dmc_model.build_vocab(corpus)  # includes null_word (others don't)\n",
      "#dbow_model.build_vocab(corpus)\n",
      "#dm_model.build_vocab(corpus)\n",
      "# avoid duplication of work/memory: copy vocab results to other models\n",
      "#dm_model.vocab, dm_model.index2word, dm_model.table = dbow_model.vocab, dbow_model.index2word, dbow_model.table\n",
      "#dm_model.vocab, dm_model.index2word, dm_model.table = dmc_model.vocab, dmc_model.index2word, dmc_model.table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dm_model.reset_weights()\n",
      "#dbow_model.vocab, dbow_model.index2word, dbow_model.table = dmc_model.vocab, dmc_model.index2word, dmc_model.table\n",
      "#dbow_model.reset_weights()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dm_model_neg_preload.merge_word2vec_format('/Users/scratch/Documents/dev2015/GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
      "#import numpy as np\n",
      "#np.copyto(dm_model_neg_punlocked.syn0,dm_model_neg_preload.syn0)\n",
      "#dm_model_neg_punlocked.syn0locks.fill(1.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#for i in range(len(dm_model_neg_preload.vocab)):\n",
      "#    word = dm_model_neg_preload.index2word[i]\n",
      "#    if not word.startswith(\"PHR_\") and dm_model_neg_preload.syn0locks[i] == 0:\n",
      "#        print(\"%d: %s\" % (i,word))\n",
      "#        break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dm_model_neg_preload.index2word[102]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print([dm_model_neg_preload.syn0[0,0],dm_model_neg_preload.syn0[134,0]])\n",
      "#print([dm_model_neg_punlocked.syn0[0,0],dm_model_neg_punlocked.syn0[134,0]])\n",
      "\n",
      "#locked6 = dbow_model.syn0[6][:]\n",
      "#unlocked29 = dbow_model.syn0[29][:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print(str( ((dbow_model.syn0[6]==locked6).all(),(dbow_model.syn0[29]==unlocked29).all())))\n",
      "#dm_model.merge_word2vec_format('/Users/scratch/Documents/dev2015/GoogleNews-vectors-negative300.bin.gz',binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dm_model.syn0[:] = dbow_model.syn0\n",
      "#dbow_model.syn0locks[6]=1.0\n",
      "\n",
      "# remember a word\n",
      "#plot_pre_training = dbow_model['plot'][:]\n",
      "#plot_pre_training\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#dbow_model.most_similar([plot_pre_training])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Evaluate a model's bulk-trained or inferred doc vectors for coarse-sentiment prediction value. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from numpy import concatenate\n",
      "import statsmodels.api as sm\n",
      "\n",
      "def error_rate_for_model(model_list, corpus, infer=False, infer_steps=3, infer_alpha=0.025):\n",
      "    \"\"\"Report error rate on SU Sentiment Treebank TEST sentences for given models/model-concatenations.\"\"\"\n",
      "    # train data: just the full TRAIN sentences\n",
      "    train_sentences = [sentence for sentence in corpus.split(corpus.TRAIN) if 'PHR_%i'%sentence.id in model_list[0].vocab]\n",
      "    train_coarse_sentiments = np.array([round(sentence.sentiment,0) for sentence in train_sentences])\n",
      "    \n",
      "    # bulk-learned doc vectors for TRAIN sentences\n",
      "    train_regressors = concatenate([[model['PHR_%i'%sentence.id] for sentence in train_sentences] for model in model_list], axis=1)\n",
      "    train_regressors = sm.add_constant(train_regressors)\n",
      "\n",
      "    # logistic regression\n",
      "    logit = sm.Logit(train_coarse_sentiments, train_regressors)\n",
      "    logit_result = logit.fit(disp=0)\n",
      "    #print(logit_result.summary())\n",
      "    \n",
      "    # test data & vectors \n",
      "    if infer:\n",
      "        test_sentences = corpus.split(corpus.TEST)  # use all: no dependence on prior training\n",
      "        test_regressors = concatenate([[model.infer_vector(sentence.text.split(),steps=infer_steps,alpha=infer_alpha) for sentence in test_sentences] for model in model_list], axis=1)\n",
      "    else:\n",
      "        # use vectors left-over from bulk training\n",
      "        test_sentences = [sentence for sentence in corpus.split(corpus.TEST) if 'PHR_%i'%sentence.id in model_list[0].vocab]\n",
      "        test_regressors = concatenate([[model['PHR_%i'%sentence.id] for sentence in test_sentences] for model in model_list], axis=1)\n",
      "    test_regressors = sm.add_constant(test_regressors)\n",
      "    test_coarse_sentiments = [round(sentence.sentiment,0) for sentence in test_sentences]\n",
      "\n",
      "    # predict & evaluate\n",
      "    test_predictions = logit_result.predict(test_regressors)\n",
      "    error_rate = sum([1 for pair in zip(test_coarse_sentiments,[round(predict,0) for predict in test_predictions]) if pair[0] != pair[1]]) / len(test_predictions)\n",
      "    return error_rate\n",
      "\n",
      "from collections import defaultdict\n",
      "best_error = defaultdict(lambda :1.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim.models.doc2vec\n",
      "gensim.models.doc2vec.FAST_VERSION"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bulk Training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use manual multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) \u2013 with added shuffling of corpus on each pass.\n",
      "\n",
      "Note that training is occurring on *all* phrases of the dataset, which includes all TRAIN/TEST/DEV sentences and their subphrases. (The subphrases aren't marked as TRAIN/TEST/DEV or with their source sentence - for now skipping the (239K phrases * 12K sentences) substring search that'd be required to split.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from random import shuffle\n",
      "import datetime\n",
      "\n",
      "alpha = 0.025 # 0.025\n",
      "passes = 40\n",
      "alpha_delta = (alpha - 0.0001) / passes\n",
      "assess_per = 1 # assess only once per 5\n",
      "infer_per = 10  # inference quite slow: only do every 10th pass\n",
      "infer_steps = 3\n",
      "infer_alpha = 0.1\n",
      "infer_subsample = 0.2\n",
      "\n",
      "print(\"START \"+str(datetime.datetime.now()))\n",
      "\n",
      "for epoch in range(passes):\n",
      "    shuffle(alldocs)\n",
      "    \n",
      "    for name, model in models_by_name.items():\n",
      "        \n",
      "        if len(model) == 1:\n",
      "            # only train the pure models\n",
      "            model[0].alpha = alpha\n",
      "            model[0].min_alpha = alpha\n",
      "            model[0].train(alldocs)\n",
      "        if (epoch+1) % assess_per == 0:\n",
      "            err = error_rate_for_model(model,corpus)\n",
      "            if err < best_error[name]:\n",
      "                best_error[name] = err\n",
      "                print(\"%f : %i passes : %s\"%(err,epoch+1,name))\n",
      "        if epoch == 0 or (epoch+1) % infer_per == 0:\n",
      "            infer_err = error_rate_for_model(model,corpus,infer=True,infer_steps=infer_steps,infer_alpha=infer_alpha)\n",
      "            if infer_err < best_error[name+'_inferred']:\n",
      "                best_error[name+'_inferred'] = infer_err\n",
      "                print(\"%f : %i passes : %s\"%(infer_err,epoch+1,name+'_inferred'))\n",
      "\n",
      "    #print(str( dbow_model.syn0locks[[6,29]] ) )\n",
      "    #print(str( (dbow_model.syn0[6,0],dbow_model.syn0[29,0] )))\n",
      "    \n",
      "    #print([dm_model_neg_preload.syn0[0,0],dm_model_neg_preload.syn0[134,0]])\n",
      "    #print([dm_model_neg_punlocked.syn0[0,0],dm_model_neg_punlocked.syn0[134,0]])\n",
      "\n",
      "    print('completed pass %i at alpha %f'%(epoch+1,alpha))\n",
      "    alpha -= alpha_delta\n",
      "    \n",
      "print(\"END \"+str(datetime.datetime.now()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "START 2015-05-03 16:58:56.644612\n",
        "0.496833 : 1 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.508145 : 1 passes : dbow_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.460633 : 1 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.427149 : 1 passes : dbow_100_neg5hs_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.463801 : 1 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.423077 : 1 passes : dm_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.423982 : 1 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.429412 : 1 passes : dm_100_neg5hs_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 1 at alpha 0.025000\n",
        "0.476471 : 2 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.406335 : 2 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.444796 : 2 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.412217 : 2 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 2 at alpha 0.024377\n",
        "0.449774 : 3 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.376018 : 3 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.411312 : 3 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.407692 : 3 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 3 at alpha 0.023755\n",
        "0.430317 : 4 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.371493 : 4 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.402715 : 4 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 4 at alpha 0.023132\n",
        "0.414480 : 5 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.398190 : 5 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 5 at alpha 0.022510\n",
        "0.367873 : 6 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.397285 : 6 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.391855 : 6 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 6 at alpha 0.021887\n",
        "0.391403 : 7 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.395928 : 7 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 7 at alpha 0.021265"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.391855 : 8 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 8 at alpha 0.020642"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.379186 : 9 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.388235 : 9 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 9 at alpha 0.020020"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.364706 : 10 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.385068 : 10 passes : dbow_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.366516 : 10 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.395928 : 10 passes : dbow_100_neg5hs_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.385520 : 10 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.408145 : 10 passes : dm_100_neg5hs_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 10 at alpha 0.019397\n",
        "0.354751 : 11 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.376923 : 11 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.391403 : 11 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 11 at alpha 0.018775\n",
        "0.352036 : 12 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.390045 : 12 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 12 at alpha 0.018152\n",
        "0.346606 : 13 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.386425 : 13 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 13 at alpha 0.017530\n",
        "0.373756 : 14 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 14 at alpha 0.016907"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 15 at alpha 0.016285"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.341176 : 16 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.365611 : 16 passes : dbow_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.363348 : 16 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 16 at alpha 0.015662"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.335747 : 17 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.362896 : 17 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 17 at alpha 0.015040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.331674 : 18 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 18 at alpha 0.014417"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.326244 : 19 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.357919 : 19 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 19 at alpha 0.013795"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.324887 : 20 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.380543 : 20 passes : dbow_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.377828 : 20 passes : dbow_100_neg5hs_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.419457 : 20 passes : dm_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 20 at alpha 0.013172"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 21 at alpha 0.012550"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.323077 : 22 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.353394 : 22 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 22 at alpha 0.011927"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 23 at alpha 0.011305"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.351131 : 24 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 24 at alpha 0.010682"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.349774 : 25 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.384163 : 25 passes : dm_100_neg5hs"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 25 at alpha 0.010060\n",
        "completed pass 26 at alpha 0.009437"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 27 at alpha 0.008815"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.348416 : 28 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 28 at alpha 0.008192"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.322172 : 29 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.344796 : 29 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 29 at alpha 0.007570"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.320814 : 30 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.374208 : 30 passes : dbow_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.416290 : 30 passes : dm_100_neg5_inferred"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 30 at alpha 0.006947"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.319457 : 31 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 31 at alpha 0.006325"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.318100 : 32 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 32 at alpha 0.005702"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 33 at alpha 0.005080"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 34 at alpha 0.004457"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.315385 : 35 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 35 at alpha 0.003835"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 36 at alpha 0.003212"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.314932 : 37 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.342534 : 37 passes : dm_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 37 at alpha 0.002590"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.314480 : 38 passes : dbow_100_neg5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 38 at alpha 0.001967"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 39 at alpha 0.001345"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 40 at alpha 0.000722"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "END 2015-05-03 17:37:24.296548\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "errs = [(rate,name) for name, rate in best_error.items()]\n",
      "errs.sort(key=lambda pair: pair[0])\n",
      "for err in errs:\n",
      "    print(\"%f %s\"%(err[0],err[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.314480 dbow_100_neg5\n",
        "0.342534 dm_100_neg5\n",
        "0.365611 dbow_100_neg5hs\n",
        "0.374208 dbow_100_neg5_inferred\n",
        "0.377828 dbow_100_neg5hs_inferred\n",
        "0.384163 dm_100_neg5hs\n",
        "0.408145 dm_100_neg5hs_inferred\n",
        "0.416290 dm_100_neg5_inferred\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "err for me\n",
      "print((dbow_model['plot']==plot_pre_training).all())\n",
      "dbow_model.most_similar('plot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#RESUME\n",
      "infer_steps = 300\n",
      "for epoch in range(43,passes):\n",
      "    shuffled = list(corpus)\n",
      "    shuffle(shuffled)\n",
      "    \n",
      "    for name, model in models_by_name.items():\n",
      "        \n",
      "        if len(model) == 1:\n",
      "            # only train the pure models\n",
      "            model[0].alpha = alpha\n",
      "            model[0].min_alpha = alpha\n",
      "            model[0].train(shuffled)\n",
      "        if (epoch+1) % assess_per == 0:\n",
      "            err = error_rate_for_model(model,corpus)\n",
      "            if err < best_error[name]:\n",
      "                best_error[name] = err\n",
      "                print(\"%f : %i passes : %s\"%(err,epoch+1,name))\n",
      "        if (epoch+1) % infer_per == 0:\n",
      "            infer_err = error_rate_for_model(model,corpus,infer=True,steps=infer_steps)\n",
      "            if infer_err < best_error[name+'_inferred']:\n",
      "                best_error[name+'_inferred'] = infer_err\n",
      "                print(\"%f : %i passes : %s\"%(infer_err,epoch+1,name+'_inferred'))\n",
      "\n",
      "    print('completed pass %i at alpha %f'%(epoch+1,alpha))\n",
      "    alpha -= alpha_delta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dbow_model.index2word[0]\n",
      "(unlocked0 is dbow_model.syn0[0])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After hundreds of passes, I've seen the DBOW model reach an error of about 27% and the DM model reach around 30%. The DM_CONCAT model is still 40%+ after a dozen or more passes (which is usually enough to get the other models into the low 30s). These are still very far from the paper's reported 12.2% error rate. Theories include:\n",
      "\n",
      "* the much larger PV-DM/concatenation model may need many, many more training passes to get good results? if so, impractical until cythonized/BLASized \n",
      "* paper *maybe* used pretrained word vectors from a much larger corpus?\n",
      "* other differences in training parameter choices/passes/corpus-cleanup?\n",
      "* an implementation flaw? the new DM_CONCAT code is nowhere near the paper's claim that \"PV-DM alone usually works well for most tasks (with state-of-art performances)\". But also, \"combination with PV-DBOW\" (as the paper recommends) is far from helping much, either, on these trials. Still the doc vectors have *some* predictive power, improving over time, and show many of the patterns in word and doc similarity relationships that would be expected from meaningful training."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Miscellaneous"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Word2Vec\n",
      "g100b_model = Word2Vec.load_word2vec_format('/Users/scratch/Documents/dev2015/GoogleNews-vectors-negative300.bin.gz',binary=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do the nearby words make sense?\n",
      "from IPython.display import HTML\n",
      "word = 'plot'\n",
      "similars_per_model = [str(model.most_similar(word)).replace('), ','),<br>\\n') for model in [dbow_model, dm_model, g100b_model]]\n",
      "similar_table = (\"<table><tr><th>DBOW</th><th>DM</th><th>g100b</th></tr><tr><td>\" + \n",
      "    \"</td><td>\".join(similars_per_model) +\n",
      "    \"</td></tr></table>\")\n",
      "print(\"most similar words for %s\"%word)\n",
      "HTML(similar_table)\n",
      "# IMO all kind of weak compared to larger datasets; DMC/neg looked best after ~13 equal passes "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word = 'good'\n",
      "similars_per_model = [str([w for w in model.most_similar(word,topn=100) if not w[0].startswith('PHR_')]).replace('), ','),<br>\\n') for model in [dbow_model, dm_model, g100b_model]]\n",
      "similar_table = (\"<table><tr><th>DBOW</th><th>DM</th><th>g100b</th></tr><tr><td style='vertical-align:top'>\" + \n",
      "    \"</td><td style='vertical-align:top'>\".join(similars_per_model) +\n",
      "    \"</td></tr></table>\")\n",
      "print(\"most similar words for %s\"%word)\n",
      "HTML(similar_table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what are nearby bulk-trained vectors for an inferred vector?\n",
      "phrase = corpus.split(corpus.TEST)[0]\n",
      "tokens = phrase.text.split(' ')\n",
      "similars_per_model = [str(model.most_similar([model.infer_vector(tokens,steps=1000,alpha=0.1)])).replace('), ','),<br>\\n') for model in [dbow_model, dm_model, g100b_model]]\n",
      "print(\"most similar docs for %i: '%s'\" %(phrase.id, phrase.text))\n",
      "similar_table = (\"<table><tr><th>DBOW</th><th>DM</th><th>g100b</th></tr><tr><td>\" + \n",
      "    \"</td><td>\".join(similars_per_model) +\n",
      "    \"</td></tr></table>\")\n",
      "HTML(similar_table)\n",
      "# in DBOW, often phrase itself or subphrase... less consistent DM/DM_CONCAT"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# viewing a phrase by id\n",
      "' '.join(corpus[144249].words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(g100b_model.vocab)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "not_in_g100b = []\n",
      "overlap_g100b = []\n",
      "for k in dbow_model.vocab:\n",
      "    if k.startswith('PHR_'):\n",
      "        continue\n",
      "    if k not in g100b_model.vocab:\n",
      "        not_in_g100b.append(k)\n",
      "    else:\n",
      "        overlap_g100b.append(k)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(not_in_g100b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(overlap_g100b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "not_in_g100b[-100:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"G100B_words.txt\",'w') as f:\n",
      "    for k in g100b_model.vocab:\n",
      "        f.write(str(k))\n",
      "        f.write('\\n')\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del g100b_model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}