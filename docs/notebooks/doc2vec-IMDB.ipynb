{
 "metadata": {
  "name": "",
  "signature": "sha256:3774d8aad3c7a1b4a207c1d4313427a78e834e7b8a3686a6cc142de169daf17b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "gensim doc2vec & IMDB sentiment dataset"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Fetch and prep exactly as in Mikolov's go.sh shell script. (Note this cell tests for existence of required files, so steps won't repeat once the final summary file (`aclImdb/alldata-id.txt`) is available alongside this notebook.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# adapted from Mikolov's example go.sh script: \n",
      "if [ ! -f \"aclImdb/alldata-id.txt\" ]\n",
      "then\n",
      "    if [ ! -d \"aclImdb\" ] \n",
      "    then\n",
      "        if [ ! -f \"aclImdb_v1.tar.gz\" ]\n",
      "        then\n",
      "          wget -quiet http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "        fi\n",
      "      tar xf aclImdb_v1.tar.gz\n",
      "    fi\n",
      "    \n",
      "  #this function will convert text to lowercase and will disconnect punctuation and special symbols from words\n",
      "  function normalize_text {\n",
      "    awk '{print tolower($0);}' < $1 | sed -e 's/\\./ \\. /g' -e 's/<br \\/>/ /g' -e 's/\"/ \" /g' \\\n",
      "    -e 's/,/ , /g' -e 's/(/ ( /g' -e 's/)/ ) /g' -e 's/\\!/ \\! /g' -e 's/\\?/ \\? /g' \\\n",
      "    -e 's/\\;/ \\; /g' -e 's/\\:/ \\: /g' > $1-norm\n",
      "  }\n",
      "\n",
      "  export LC_ALL=C\n",
      "  for j in train/pos train/neg test/pos test/neg train/unsup; do\n",
      "    rm temp\n",
      "    for i in `ls aclImdb/$j`; do cat aclImdb/$j/$i >> temp; awk 'BEGIN{print;}' >> temp; done\n",
      "    normalize_text temp\n",
      "    mv temp-norm aclImdb/$j/norm.txt\n",
      "  done\n",
      "  mv aclImdb/train/pos/norm.txt aclImdb/train-pos.txt\n",
      "  mv aclImdb/train/neg/norm.txt aclImdb/train-neg.txt\n",
      "  mv aclImdb/test/pos/norm.txt aclImdb/test-pos.txt\n",
      "  mv aclImdb/test/neg/norm.txt aclImdb/test-neg.txt\n",
      "  mv aclImdb/train/unsup/norm.txt aclImdb/train-unsup.txt\n",
      "\n",
      "  cat aclImdb/train-pos.txt aclImdb/train-neg.txt aclImdb/test-pos.txt aclImdb/test-neg.txt aclImdb/train-unsup.txt > aclImdb/alldata.txt\n",
      "  awk 'BEGIN{a=0;}{print \"_*\" a \" \" $0; a++;}' < aclImdb/alldata.txt > aclImdb/alldata-id.txt\n",
      "fi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os.path\n",
      "assert os.path.isfile(\"aclImdb/alldata-id.txt\"), \"alldata-id.txt unavailable\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The data is small enough to be read into memory. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models.doc2vec import TaggedDocument\n",
      "from collections import namedtuple\n",
      "\n",
      "SentimentDocument = namedtuple('SentimentDocument','words tags split sentiment')\n",
      "\n",
      "alldocs = []  # will hold all docs in original order\n",
      "with open('aclImdb/alldata-id.txt') as alldata:\n",
      "    for line_no, line in enumerate(alldata):\n",
      "        tokens = line.split()\n",
      "        words = tokens[1:]\n",
      "        tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
      "        split = ['train','test','extra','extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
      "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
      "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
      "\n",
      "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
      "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
      "doc_list = alldocs[:]  # for reshuffling per pass\n",
      "\n",
      "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set-up Doc2Vec Training & Evaluation Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Approximating experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), also with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
      "\n",
      "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
      "\n",
      "Parameter choices below vary:\n",
      "\n",
      "* 100-dimensional vectors, as the 400d vectors of the paper don't seem to offer much benefit on this task\n",
      "* similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
      "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
      "* added to that DBOW model two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger model)\n",
      "* a `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Doc2Vec\n",
      "import gensim.models.doc2vec\n",
      "from collections import OrderedDict\n",
      "import multiprocessing\n",
      "\n",
      "cores = multiprocessing.cpu_count()\n",
      "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
      "\n",
      "simple_models = [\n",
      "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
      "    Doc2Vec(dm=1,dm_concat=1,size=100,window=5,negative=5,hs=0,min_count=2,workers=cores),\n",
      "    # PV-DBOW \n",
      "    Doc2Vec(dm=0,size=100,negative=5,hs=0,min_count=2,workers=cores),\n",
      "    # PV-DM w/average\n",
      "    Doc2Vec(dm=1,dm_mean=1,size=100,window=10,negative=5,hs=0,min_count=2,workers=cores),\n",
      "]\n",
      "\n",
      "# speed setup by sharing results of 1st model's vocabulary scan\n",
      "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
      "print(simple_models[0].compact_name)\n",
      "for model in simple_models[1:]:\n",
      "    model.reset_from(simple_models[0])\n",
      "    print(model.compact_name)\n",
      "\n",
      "models_by_name = OrderedDict((model.compact_name, model) for model in simple_models)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dmc_d100n5w5mc2t4\n",
        "dbow_d100n5mc2t4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dmm_d100n5w10mc2t4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following the paper, we also evaluate models in pairs. These wrappers return the concatenation of the vectors from each model. (Only the singular models are trained.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
      "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([models_by_name['dbow_d100n5mc2t4'], models_by_name['dmm_d100n5w10mc2t4']])\n",
      "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([models_by_name['dbow_d100n5mc2t4'], models_by_name['dmc_d100n5w5mc2t4']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Predictive Evaluation Methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Helper methods for evaluating error rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import statsmodels.api as sm\n",
      "from random import sample\n",
      "\n",
      "# for timing\n",
      "from contextlib import contextmanager\n",
      "from timeit import default_timer\n",
      "import time \n",
      "\n",
      "@contextmanager\n",
      "def elapsed_timer():\n",
      "    start = default_timer()\n",
      "    elapser = lambda: default_timer() - start\n",
      "    yield lambda: elapser()\n",
      "    end = default_timer()\n",
      "    elapser = lambda: end-start\n",
      "    \n",
      "def logistic_predictor_from_data(train_targets, train_regressors):\n",
      "    logit = sm.Logit(train_targets, train_regressors)\n",
      "    predictor = logit.fit(disp=0)\n",
      "    #print(predictor.summary())\n",
      "    return predictor\n",
      "\n",
      "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
      "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
      "\n",
      "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
      "    train_regressors = sm.add_constant(train_regressors)\n",
      "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
      "\n",
      "    test_data = test_set\n",
      "    if infer:\n",
      "        if infer_subsample < 1.0:\n",
      "            test_data = sample(test_data, int(infer_subsample*len(test_data)))\n",
      "        test_regressors = [test_model.infer_vector(doc.words,steps=infer_steps,alpha=infer_alpha) for doc in test_data]\n",
      "    else:\n",
      "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
      "    test_regressors = sm.add_constant(test_regressors)\n",
      "    \n",
      "    # predict & evaluate\n",
      "    test_predictions = predictor.predict(test_regressors)\n",
      "    corrects = sum(np.rint(test_predictions)==[doc.sentiment for doc in test_data])\n",
      "    errors = len(test_predictions) - corrects\n",
      "    error_rate = float(errors) / len(test_predictions)\n",
      "    return (error_rate, errors, len(test_predictions), predictor)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bulk Training"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using explicit multiple-pass, alpha-reduction approach as sketched in [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) \u2013 with added shuffling of corpus on each pass.\n",
      "\n",
      "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
      "\n",
      "Evaluation of each model's sentiment-predictive power is repeated after each pass, as an error rate (lower is better), to see the rates-of-relative-improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. \n",
      "\n",
      "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3 main models takes about an hour.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import shuffle\n",
      "import datetime\n",
      "\n",
      "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
      "alpha_delta = (alpha - min_alpha) / passes\n",
      "\n",
      "print(\"START %s\" % datetime.datetime.now())\n",
      "\n",
      "for epoch in range(passes):\n",
      "    shuffle(doc_list)  # shuffling gets best results\n",
      "    \n",
      "    for name, train_model in models_by_name.items():\n",
      "        # train\n",
      "        duration = 'na'\n",
      "        train_model.alpha, train_model.min_alpha = (alpha, alpha)\n",
      "        with elapsed_timer() as elapsed:\n",
      "            train_model.train(doc_list)\n",
      "            duration = '%.1f' % elapsed()\n",
      "            \n",
      "        # evaluate\n",
      "        eval_duration = ''\n",
      "        with elapsed_timer() as eval_elapsed:\n",
      "            (err, err_count, test_count, predictor) = error_rate_for_model(train_model, train_docs, test_docs)\n",
      "        eval_duration = '%.1f' % eval_elapsed()\n",
      "        if err < best_error[name]:\n",
      "            best_error[name] = err\n",
      "            print(\"%f : %i passes : %s %ss %ss\"%(err,epoch+1,name, duration, eval_duration))\n",
      "\n",
      "        eval_duration = ''\n",
      "        with elapsed_timer() as eval_elapsed:\n",
      "            (infer_err, err_count, test_count, predictor) = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
      "        eval_duration = '%.1f' % eval_elapsed()\n",
      "        if infer_err < best_error[name+'_inferred']:\n",
      "            best_error[name+'_inferred'] = infer_err\n",
      "            print(\"%f : %i passes : %s %ss %ss\"%(infer_err,epoch+1,name+'_inferred', duration, eval_duration))\n",
      "\n",
      "    print('completed pass %i at alpha %f'%(epoch+1,alpha))\n",
      "    alpha -= alpha_delta\n",
      "    \n",
      "print(\"END %s\" % str(datetime.datetime.now()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "START 2015-06-10 05:01:59.714002\n",
        "0.418360 : 1 passes : dmc_d100n5w5mc2t4 66.0s 1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.389600 : 1 passes : dmc_d100n5w5mc2t4_inferred 66.0s 10.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.221280 : 1 passes : dbow_d100n5mc2t4 28.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.193600 : 1 passes : dbow_d100n5mc2t4_inferred 28.6s 5.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.276920 : 1 passes : dmm_d100n5w10mc2t4 37.8s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.225200 : 1 passes : dmm_d100n5w10mc2t4_inferred 37.8s 6.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.207720 : 1 passes : dbow+dmm 0.0s 2.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.188400 : 1 passes : dbow+dmm_inferred 0.0s 12.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.221440 : 1 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.218000 : 1 passes : dbow+dmc_inferred 0.0s 17.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 1 at alpha 0.025000\n",
        "0.364360 : 2 passes : dmc_d100n5w5mc2t4 59.6s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.368000 : 2 passes : dmc_d100n5w5mc2t4_inferred 59.6s 10.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.140840 : 2 passes : dbow_d100n5mc2t4 28.8s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.166000 : 2 passes : dbow_d100n5mc2t4_inferred 28.8s 5.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.223720 : 2 passes : dmm_d100n5w10mc2t4 35.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.198000 : 2 passes : dmm_d100n5w10mc2t4_inferred 35.6s 6.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.136240 : 2 passes : dbow+dmm 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.163600 : 2 passes : dbow+dmm_inferred 0.0s 12.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.140840 : 2 passes : dbow+dmc 0.0s 2.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.176400 : 2 passes : dbow+dmc_inferred 0.0s 16.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 2 at alpha 0.023800\n",
        "0.331400 : 3 passes : dmc_d100n5w5mc2t4 59.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.357600 : 3 passes : dmc_d100n5w5mc2t4_inferred 59.1s 10.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.122240 : 3 passes : dbow_d100n5mc2t4 29.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.124000 : 3 passes : dbow_d100n5mc2t4_inferred 29.0s 5.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.200240 : 3 passes : dmm_d100n5w10mc2t4 34.8s 1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.120560 : 3 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.129200 : 3 passes : dbow+dmm_inferred 0.0s 11.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.122480 : 3 passes : dbow+dmc 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.144400 : 3 passes : dbow+dmc_inferred 0.0s 16.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 3 at alpha 0.022600\n",
        "0.307800 : 4 passes : dmc_d100n5w5mc2t4 56.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.336000 : 4 passes : dmc_d100n5w5mc2t4_inferred 56.1s 11.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.113840 : 4 passes : dbow_d100n5mc2t4 31.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.122400 : 4 passes : dbow_d100n5mc2t4_inferred 31.6s 5.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.187000 : 4 passes : dmm_d100n5w10mc2t4 34.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.112880 : 4 passes : dbow+dmm 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.124400 : 4 passes : dbow+dmm_inferred 0.0s 13.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.114040 : 4 passes : dbow+dmc 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.128800 : 4 passes : dbow+dmc_inferred 0.0s 16.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 4 at alpha 0.021400\n",
        "0.283640 : 5 passes : dmc_d100n5w5mc2t4 59.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.325600 : 5 passes : dmc_d100n5w5mc2t4_inferred 59.6s 10.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.109600 : 5 passes : dbow_d100n5mc2t4 32.6s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.112800 : 5 passes : dbow_d100n5mc2t4_inferred 32.6s 6.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.180760 : 5 passes : dmm_d100n5w10mc2t4 44.2s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.191200 : 5 passes : dmm_d100n5w10mc2t4_inferred 44.2s 6.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.108600 : 5 passes : dbow+dmm 0.0s 1.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.120000 : 5 passes : dbow+dmm_inferred 0.0s 12.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.109720 : 5 passes : dbow+dmc 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.118000 : 5 passes : dbow+dmc_inferred 0.0s 18.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 5 at alpha 0.020200\n",
        "0.270080 : 6 passes : dmc_d100n5w5mc2t4 70.3s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.308000 : 6 passes : dmc_d100n5w5mc2t4_inferred 70.3s 11.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.106120 : 6 passes : dbow_d100n5mc2t4 33.9s 1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.175800 : 6 passes : dmm_d100n5w10mc2t4 45.1s 1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.106520 : 6 passes : dbow+dmm 0.0s 1.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.113200 : 6 passes : dbow+dmm_inferred 0.0s 12.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.106600 : 6 passes : dbow+dmc 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 6 at alpha 0.019000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.256880 : 7 passes : dmc_d100n5w5mc2t4 67.4s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.283600 : 7 passes : dmc_d100n5w5mc2t4_inferred 67.4s 11.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.105000 : 7 passes : dbow_d100n5mc2t4 34.8s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.172520 : 7 passes : dmm_d100n5w10mc2t4 44.5s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.104240 : 7 passes : dbow+dmm 0.0s 1.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.111200 : 7 passes : dbow+dmm_inferred 0.0s 13.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.106240 : 7 passes : dbow+dmc 0.0s 1.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.116400 : 7 passes : dbow+dmc_inferred 0.0s 16.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 7 at alpha 0.017800\n",
        "0.249680 : 8 passes : dmc_d100n5w5mc2t4 71.7s 1.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.266400 : 8 passes : dmc_d100n5w5mc2t4_inferred 71.7s 11.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.109200 : 8 passes : dbow_d100n5mc2t4_inferred 34.7s 6.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.168840 : 8 passes : dmm_d100n5w10mc2t4 46.3s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.103520 : 8 passes : dbow+dmm 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.105320 : 8 passes : dbow+dmc 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 8 at alpha 0.016600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.240200 : 9 passes : dmc_d100n5w5mc2t4 61.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.257600 : 9 passes : dmc_d100n5w5mc2t4_inferred 61.9s 10.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.104640 : 9 passes : dbow_d100n5mc2t4 28.5s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.107200 : 9 passes : dbow_d100n5mc2t4_inferred 28.5s 5.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.166800 : 9 passes : dmm_d100n5w10mc2t4 35.7s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.104200 : 9 passes : dbow+dmc 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.107200 : 9 passes : dbow+dmc_inferred 0.0s 16.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 9 at alpha 0.015400\n",
        "0.236040 : 10 passes : dmc_d100n5w5mc2t4 68.6s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.253200 : 10 passes : dmc_d100n5w5mc2t4_inferred 68.6s 10.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.102960 : 10 passes : dbow_d100n5mc2t4 34.6s 1.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.095600 : 10 passes : dbow_d100n5mc2t4_inferred 34.6s 6.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.163880 : 10 passes : dmm_d100n5w10mc2t4 45.7s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.108000 : 10 passes : dbow+dmm_inferred 0.0s 12.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.104080 : 10 passes : dbow+dmc 0.0s 2.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.100000 : 10 passes : dbow+dmc_inferred 0.0s 17.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 10 at alpha 0.014200\n",
        "0.229280 : 11 passes : dmc_d100n5w5mc2t4 70.8s 0.9s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.163640 : 11 passes : dmm_d100n5w10mc2t4 34.9s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.106800 : 11 passes : dbow+dmm_inferred 0.0s 12.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 11 at alpha 0.013000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.225440 : 12 passes : dmc_d100n5w5mc2t4 58.0s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.239200 : 12 passes : dmc_d100n5w5mc2t4_inferred 58.0s 10.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.102120 : 12 passes : dbow_d100n5mc2t4 28.8s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.161880 : 12 passes : dmm_d100n5w10mc2t4 35.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.186000 : 12 passes : dmm_d100n5w10mc2t4_inferred 35.1s 6.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.102320 : 12 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.103320 : 12 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 12 at alpha 0.011800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.223600 : 13 passes : dmc_d100n5w5mc2t4 52.7s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.232400 : 13 passes : dmc_d100n5w5mc2t4_inferred 52.7s 10.2s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.161320 : 13 passes : dmm_d100n5w10mc2t4 34.4s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.102040 : 13 passes : dbow+dmm 0.0s 1.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.104000 : 13 passes : dbow+dmm_inferred 0.0s 11.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 13 at alpha 0.010600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.221040 : 14 passes : dmc_d100n5w5mc2t4 53.6s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.159560 : 14 passes : dmm_d100n5w10mc2t4 35.2s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.185600 : 14 passes : dmm_d100n5w10mc2t4_inferred 35.2s 7.1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 14 at alpha 0.009400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.217920 : 15 passes : dmc_d100n5w5mc2t4 54.3s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.159200 : 15 passes : dmm_d100n5w10mc2t4 35.3s 0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.101720 : 15 passes : dbow+dmm 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 15 at alpha 0.008200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.217440 : 16 passes : dmc_d100n5w5mc2t4 52.9s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.158640 : 16 passes : dmm_d100n5w10mc2t4 35.9s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.101280 : 16 passes : dbow+dmm 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.098000 : 16 passes : dbow+dmm_inferred 0.0s 12.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 16 at alpha 0.007000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.215960 : 17 passes : dmc_d100n5w5mc2t4 52.2s 1.4s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.101200 : 17 passes : dbow+dmm 0.0s 1.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.094800 : 17 passes : dbow+dmm_inferred 0.0s 13.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 17 at alpha 0.005800"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.229200 : 18 passes : dmc_d100n5w5mc2t4_inferred 52.6s 10.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.158440 : 18 passes : dmm_d100n5w10mc2t4 35.4s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.181600 : 18 passes : dmm_d100n5w10mc2t4_inferred 35.4s 6.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 18 at alpha 0.004600"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.215440 : 19 passes : dmc_d100n5w5mc2t4 52.5s 1.3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.217600 : 19 passes : dmc_d100n5w5mc2t4_inferred 52.5s 9.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.158280 : 19 passes : dmm_d100n5w10mc2t4 34.1s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.179600 : 19 passes : dmm_d100n5w10mc2t4_inferred 34.1s 7.0s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.100720 : 19 passes : dbow+dmm 0.0s 1.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.103240 : 19 passes : dbow+dmc 0.0s 1.5s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 19 at alpha 0.003400"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.215400 : 20 passes : dmc_d100n5w5mc2t4 50.4s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.157960 : 20 passes : dmm_d100n5w10mc2t4 33.8s 0.8s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "completed pass 20 at alpha 0.002200"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "END 2015-06-10 06:03:58.499051\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Achieved Sentiment-Prediction Accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# print best error rates achieved\n",
      "errs = [(rate,name) for name, rate in best_error.items()]\n",
      "errs.sort(key=lambda pair: pair[0])\n",
      "for err in errs:\n",
      "    print(\"%f %s\"%(err[0],err[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.094800 dbow+dmm_inferred\n",
        "0.095600 dbow_d100n5mc2t4_inferred\n",
        "0.100000 dbow+dmc_inferred\n",
        "0.100720 dbow+dmm\n",
        "0.102120 dbow_d100n5mc2t4\n",
        "0.103240 dbow+dmc\n",
        "0.157960 dmm_d100n5w10mc2t4\n",
        "0.179600 dmm_d100n5w10mc2t4_inferred\n",
        "0.215400 dmc_d100n5w5mc2t4\n",
        "0.217600 dmc_d100n5w5mc2t4_inferred\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In my testing, unlike the paper's report, DBOW performs best. Concatenating vectors from different models only offers a small improvement. The best results I've seen are still just under 10% error rate, still a ways from the paper's 7.42%. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Examining Results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Are inferred vectors close to the precalculated ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
      "print('for doc %d...' % doc_id)\n",
      "for model in simple_models:\n",
      "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
      "    print('%s: %s' % (model.compact_name, model.docvecs.most_similar([inferred_docvec],topn=3)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "for doc 59538...\n",
        "dmc_d100n5w5mc2t4: [(59538, 0.7505937814712524), (19191, 0.4108924865722656), (59704, 0.40968963503837585)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dbow_d100n5mc2t4: [(59538, 0.9496155977249146), (63920, 0.6974467635154724), (45836, 0.6542057394981384)]\n",
        "dmm_d100n5w10mc2t4: [(59538, 0.8298757076263428), (28388, 0.8006762266159058), (70797, 0.7880938053131104)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated \u2013 just 3 steps starting at a high alpha \u2013 and likely need tuning for other applications.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do close documents seem more related than distant ones?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
      "model = random.choice(simple_models)  # and a random model\n",
      "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
      "print('TARGET (%d): \u00ab%s\u00bb\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
      "print('SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model.compact_name)\n",
      "for label, index in [('MOST',0), ('MEDIAN',len(sims)//2), ('LEAST',len(sims)-1)]:\n",
      "    print('%s %s: \u00ab%s\u00bb\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "TARGET (44092): \u00abthis is awful , you just could't believe it . the score is annoying , the filming is bad , for example , sometimes you see the shadow of the cameraman appearing on some actors faces . the quality of the movie is ultra bad , seems like it was made in the 20ies . it's terrible . there is a bit of blood in the beginning and through the movie but always too dark filmed . no gore no effects . the director made some better one like blood rites . but out there there is a following of the man , 'cause searching to find this cheap flick isn't that hard but you have to pay hard earned cash for it . surely this will get in my top ten of worst horrormovies ever , i don't know if i would call it horror . there is too much talking , you will get bloodthirsty after watching it\u00bb\n",
        "\n",
        "SIMILAR/DISSIMILAR DOCS PER MODEL dmm_d100n5w10mc2t4:\n",
        "\n",
        "MOST (13474, 0.7617485523223877): \u00abthis film is so much of a rip-off of the masterpeice \" demons \" and thats the only thing that makes the movie worth watching . the acting is terrible , the action scenes are speeded up , the script is almost painful and budget non existent . if you think this film is good then you havn't seen a real horror film , skip this and get a copy of the movie demons .\u00bb\n",
        "\n",
        "MEDIAN (68465, 0.41534656286239624): \u00abfor over 1000 years beowulf the epic has described beowulf as a mighty hero who killed grendel and grendel's mother . he became a mighty king in his own right after protecting the existing incumbent and his son . at the end of a life of courage and honesty he sets out to fight one last battle , knowing he may be going to his death but willing to protect his people for one last time . during the final battle with the fire spewing serpent , beowulf was losing , but one of his companions remembered his duty and where others deserted beowulf , wiglaf returned to stand by his dying lord , shielding him and dealing a stroke that abated the serpent's fire , enabling beowulf to deal the death stroke to the serpent with his battle knife . finally after the death of the serpent , and the subsequent death of his dear lord from his wounds , wiglaf berates the cowards who deserted their lord and made them feel their shame . a short synopsis of the epic of beowulf . what beowulf is presented in this movie ? a pervert who sleeps with a demon , holds his lands at her behest , abhors his life and spawns a mutant . wrong wrong wrong wrong wrong , so wrong it is not remotely funny . the main thing that bothers me is that those who see the film will think it is in any way accurate ( even in the fight against grendel epic beowulf stated he scorns to carry sword or shield he does mention his shirt of mail ! ) and a tale of unblemished heroism that has lasted fire and reformation over 10 centuries gets buried by a below average cgi flick with a bigger advertising budget .\u00bb\n",
        "\n",
        "LEAST (79375, -0.08628208190202713): \u00abas i watched the survivors , i couldn't help but wonder what was going through the mind of director michael ritchie when he was presented with the script . outside of the enormous gaps in plot and development , he had to see some humor in it somewhere to cast two direct opposites of the comedy spectrum to helm this project . there had to be a mission or a reason in ritchie's mind when he decided that robin williams , a fast-talking comedian that can sometimes be uncontrollable , and walter matthau , a slow-methodical comedian that appeals to the \" every man \" , would be his key players . i wish i could have been a fly on the wall during this opening meeting because this little fly would have spoken up and mentioned that this pairing would doom the script , and possibly put a black mark on both of these actor's careers . i wouldn't just stop there , i would tear this film to pieces trying to get others to explain to me the subsequent ending and missing tone . the only element that i would be content with would be the casting of jerry reed , who honestly brought some humor and intelligence to this scarred film . the survivors was not a film , but instead an attempt to allow two comedians the opportunity to express themselves coupled with heavy firepower . nothing more , nothing less . could somebody , anybody , please help me out with the story surrounding the survivors ? from the zigzag opening centered around the parrot and robin williams' job to the incident at matthau's gas station ( a plot point never mentioned or concluded ) , ritchie spends no time developing anything . his choice of direction is simply to allow williams to be as \" zany \" as possible and see how matthau reacts to it . if it weren't for jerry reed this film would have been nearly an hour and a half of forced jokes , gunshots , and awkward moments . the story was pointless . in most instances i can find bits and pieces of a story which keeps my attention allowing me to be curious about how the ending will resolve itself . for there to be this resolution , there has to be a conflict . ritchie attempts to create one with the entire \" survival of the fittest \" byline , but even that idea is never fully announced . i felt like a ping-pong ball in this film , constantly going back and forth between williams and matthau hoping that i would land on something that scored a point , but alas , this was the game that would never end . ritchie even takes us into the wilderness in attempts to bring more laughs and eventually draw an ending , but again , nothing happens . nothing is explained , nothing is developed , nothing is linear . williams goes into the woods to be trained in survival , yet for the amount of time he was there it was as if he was unable to learn anything . also , where did he get the funds to buy the house out in the woods ? then , without giving anything away , there was that pathetic ending . what happened ? i use big words there because there was not one iota of a conclusion . enemies became friends , friends became enemies , and before words could be spoken the ending credits appeared . i would like to announce this here , but i believe michael ritchie could not even handle the simplest of tasks with this film . the direction was horrible because ritchie could not control his actors . it was obvious as you watched williams and matthau on screen that there were getting no advice or pointers from the man behind the camera . ritchie didn't stop williams during his rants ( which at times were never relevant to the film ) and did not help matthau react to the insanity that williams was bringing to the table . what should have been the best part of this film was easily the most painful to watch . williams and matthau , in this critic's eye , possibly could be ranked as the worst comic pairing in cinema . matthau's form of comedy is completely , if not 100% , different to williams' shenanigans . while in some film cases this would work to a movie's advantage , for the survivors , it did not . there were no characters for these two comedians to enter into . i sat during the entire hour and a half watching robin williams be robin williams and the same for walter matthau . i could not see any semblance of a character between the two of them . both seemed to jump from one trait to the next . neither seemed to have a complete hold or knowledge of who they were attempting to portray . this is half due to the flimsy story , but mainly i place the blame on ritchie . with williams and matthau at the helm , this had the beginnings of a hilarious possible gut-busting , laugh-out-loud comedy that would be a staple in the film community , but ritchie , in my eyes , could not handle it . he relied to heavily on his actor's comic \" personas \" instead of actually building characters for them . overall , this was a very sad excuse for a film . i have read some other reviews that speak highly of the comedy in this film while do speak similarly of the lacking story , but for me everything was broken . there were no characters , there was no direction , there was obviously no story , and our two central actors didn't work for their money , but just read through their lines and gave a measly 30% to the final product . the only plus i give this film is the accomplishment of jerry reed . he was worth watching . the scene between his wife and i was nearly close to perfection . i think it was the only time that i found myself chuckling through this entire film . ritchie could not handle this film and in the end the survivors is probably a film that neither williams or matthau wants to remember . grade : ** out of *****\u00bb\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(Yes, in terms of reviewer tone, movie genre, etc... the MOST similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do the word vectors show useful similarities?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_models = simple_models[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "# pick a random word with a suitable number of occurences\n",
      "while True:\n",
      "    word = random.choice(word_models[0].index2word)\n",
      "    if word_models[0].vocab[word].count > 10:\n",
      "        break\n",
      "# or just pick a word from the relevant domain:\n",
      "# word = 'plot'\n",
      "similars_per_model = [str(model.most_similar(word,topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
      "similar_table = (\"<table><tr><th>\" +\n",
      "    \"</th><th>\".join([model.compact_name for model in word_models]) + \n",
      "    \"</th></tr><tr><td>\" +\n",
      "    \"</td><td>\".join(similars_per_model) +\n",
      "    \"</td></tr></table>\")\n",
      "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].vocab[word].count))\n",
      "HTML(similar_table)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "most similar words for 'reaction' (912 occurences)\n"
       ]
      },
      {
       "html": [
        "<table><tr><th>dmc_d100n5w5mc2t4</th><th>dbow_d100n5mc2t4</th><th>dmm_d100n5w10mc2t4</th></tr><tr><td>[('response', 0.7310047745704651),<br>\n",
        "('reactions', 0.71299809217453),<br>\n",
        "('obliviousness', 0.5911725163459778),<br>\n",
        "('objection', 0.5809897184371948),<br>\n",
        "('responses', 0.5774205327033997),<br>\n",
        "('suggestion', 0.5519779324531555),<br>\n",
        "('correlation', 0.5490224361419678),<br>\n",
        "('rationality', 0.5477378964424133),<br>\n",
        "('conclusion', 0.5427832007408142),<br>\n",
        "('aversion', 0.5426162481307983),<br>\n",
        "('reply', 0.5408373475074768),<br>\n",
        "('introduction', 0.5372020602226257),<br>\n",
        "('contribution', 0.5326882600784302),<br>\n",
        "('magnetism', 0.5296876430511475),<br>\n",
        "('aggressiveness', 0.527334988117218),<br>\n",
        "('soliloquy', 0.5229986906051636),<br>\n",
        "('precursor', 0.5208104848861694),<br>\n",
        "('apprehension', 0.5201228857040405),<br>\n",
        "('v-card', 0.5191075801849365),<br>\n",
        "('inclination', 0.5187612771987915)]</td><td>[('yol', 0.40934062004089355),<br>\n",
        "('motels', 0.4083479046821594),<br>\n",
        "('discharged', 0.38723981380462646),<br>\n",
        "('legalized', 0.38527610898017883),<br>\n",
        "('frazee', 0.37536871433258057),<br>\n",
        "(\"short's\", 0.3726074993610382),<br>\n",
        "('humberfloob', 0.3696143627166748),<br>\n",
        "(\"'chicago\", 0.3695574998855591),<br>\n",
        "('partnered', 0.36848723888397217),<br>\n",
        "('archard', 0.36810845136642456),<br>\n",
        "('tenko', 0.3663099408149719),<br>\n",
        "('concessions', 0.3662480115890503),<br>\n",
        "('policier', 0.36383259296417236),<br>\n",
        "('aide-de-camp', 0.3637371063232422),<br>\n",
        "('cates', 0.3626943826675415),<br>\n",
        "('punk', 0.36106568574905396),<br>\n",
        "('robin', 0.35913658142089844),<br>\n",
        "('geysers', 0.35744380950927734),<br>\n",
        "('cheated', 0.3571518659591675),<br>\n",
        "('surveying', 0.35693296790122986)]</td><td>[('response', 0.7806841135025024),<br>\n",
        "('reactions', 0.7511624693870544),<br>\n",
        "('introduction', 0.6644599437713623),<br>\n",
        "('dismay', 0.6599670648574829),<br>\n",
        "('face', 0.6541042327880859),<br>\n",
        "('contribution', 0.6497777700424194),<br>\n",
        "('decision', 0.6441717743873596),<br>\n",
        "('stomach', 0.6292012929916382),<br>\n",
        "('attraction', 0.6280107498168945),<br>\n",
        "('objection', 0.6244252920150757),<br>\n",
        "('reference', 0.6200262308120728),<br>\n",
        "('attachment', 0.617138147354126),<br>\n",
        "('inclination', 0.6141039133071899),<br>\n",
        "('approach', 0.6132383942604065),<br>\n",
        "('responses', 0.6101058125495911),<br>\n",
        "('suggestion', 0.6090033054351807),<br>\n",
        "('counterpoint', 0.6075911521911621),<br>\n",
        "('counter-balance', 0.6052888035774231),<br>\n",
        "('reply', 0.5980963110923767),<br>\n",
        "('transition', 0.5962440967559814)]</td></tr></table>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 93,
       "text": [
        "<IPython.core.display.HTML at 0x165827a58>"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors \u2013 they remain at their random initialized values \u2013 unless you ask with the `dbow_words=1` initialization parameter. The DBOW doc vectors can be trained faster \u2013 and are even better on tasks like IMDB sentiment-prediction \u2013 *without* simultaneous word-training.) \n",
      "\n",
      "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Are the word vectors from this dataset any good at analogies?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assuming something like\n",
      "# https://word2vec.googlecode.com/svn/trunk/questions-words.txt \n",
      "# is in local directory\n",
      "for model in word_models:\n",
      "    sections = model.accuracy('questions-words.txt')\n",
      "    correct, incorrect = (len(sum((s['correct'] for s in sections), [])), len(sum((s['incorrect'] for s in sections),[])))\n",
      "    print('%s: %0.2f%% correct (%d of %d)' % (model.compact_name, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dmc_d100n5w5mc2t4: 27.28% correct (5462 of 20024)\n",
        "dbow_d100n5mc2t4: 0.00% correct (0 of 20024)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "dmm_d100n5w10mc2t4: 27.50% correct (5506 of 20024)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even though this is a tiny, domain-specific dataset, it shows some meagher capability on the general word analogies \u2013 at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Slop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To mix the Google dataset (if locally available) into the word tests..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import Word2Vec\n",
      "w2v_g100b = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
      "w2v_g100b.compact_name = 'w2v_g100b'\n",
      "word_models.append(w2v_g100b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get copious logging output from above steps..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
      "rootLogger = logging.getLogger()\n",
      "rootLogger.setLevel(logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To auto-reload python code while developing..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}